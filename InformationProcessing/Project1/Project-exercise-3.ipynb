{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyPhrase extraction\n",
    "* Use SemEval 2010 dataset - [train](https://github.com/boudinfl/ake-datasets/blob/master/datasets/SemEval-2010/train/) dataset for TF-IDF vectorization\n",
    "* Use SemEval 2010 dataset - [test](https://github.com/boudinfl/ake-datasets/blob/master/datasets/SemEval-2010/test/) for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xml.etree import ElementTree\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(directory):\n",
    "    docs = {}\n",
    "    for doc_path in tqdm(glob(f'{directory}/*.xml')):\n",
    "        doc = ElementTree.parse(doc_path)\n",
    "        sentences = []\n",
    "        for sentence in doc.find('document').find('sentences').findall('sentence'):\n",
    "            sentences.append(' '.join([token.find('lemma').text.lower() + '~' + token.find('POS').text\n",
    "                                       for token in sentence.find('tokens').findall('token')]))\n",
    "\n",
    "        docs[doc_path.split('/')[-1].split('.')[0]] = '\\n'.join(sentences)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0794e595634f4b32b443804adca7ad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=144), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ff021146ab4ad496554a19c4c7aed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144, 100)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read('ake-datasets/datasets/SemEval-2010/train')\n",
    "test_sentences = read('ake-datasets/datasets/SemEval-2010/test')\n",
    "len(train_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(((\\w+~JJ)* (\\w+~NN)+ (\\w+~IN))?(\\w+~JJ)+ (\\w+~NN)+)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_candidates = {doc_id: [candidate[0] for candidate in re.findall(pattern, doc)] for doc_id, doc in train_sentences.items()}\n",
    "train_candidates = {doc_id: [' '.join([w.split('~')[0] for w in candidate.split()]) for candidate in candidates] for doc_id, candidates in train_candidates.items()}\n",
    "train_sentences = {doc_id: ' '.join([w.split('~')[0] for w in sentences.split()]) for doc_id, sentences in train_sentences.items()}\n",
    "train_frequencies = {doc_id: Counter(\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 1)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 2)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 3)])\n",
    "                    for doc_id, doc in train_sentences.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates = {doc_id: [candidate[0] for candidate in re.findall(pattern, doc)] for doc_id, doc in test_sentences.items()}\n",
    "test_candidates = {doc_id: [' '.join([w.split('~')[0] for w in candidate.split()]) for candidate in candidates] for doc_id, candidates in test_candidates.items()}\n",
    "test_sentences = {doc_id: ' '.join([w.split('~')[0] for w in sentences.split()]) for doc_id, sentences in test_sentences.items()}\n",
    "test_frequencies = {doc_id: Counter(\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 1)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 2)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 3)])\n",
    "                    for doc_id, doc in test_sentences.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 3))\n",
    "trainvec = vectorizer.fit_transform(train_sentences.values())\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ake-datasets/datasets/SemEval-2010/references/test.author.stem.json', 'r') as f:\n",
    "    target = json.load(f)\n",
    "    target = {doc_name: [k[0] for k in keyphrases] for doc_name, keyphrases in target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collabor target detect',\n",
       " 'deploy',\n",
       " 'exposur',\n",
       " 'sensor network',\n",
       " 'valu fusion']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['C-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collaborative target',\n",
       " 'certain level',\n",
       " 'sequential deployment',\n",
       " 'recent advance',\n",
       " 'specific event',\n",
       " 'specific region',\n",
       " 'such system',\n",
       " 'civil infrastructure',\n",
       " 'other means',\n",
       " 'random placement',\n",
       " 'local observation',\n",
       " 'global decision',\n",
       " 'local processing',\n",
       " 'different node',\n",
       " 'local observation',\n",
       " 'possible measure',\n",
       " 'practical application',\n",
       " 'precise placement',\n",
       " 'random deployment',\n",
       " 'several solution',\n",
       " 'analytical study',\n",
       " 'optimum solution',\n",
       " 'rectangular sensor',\n",
       " 'geometric distance',\n",
       " 'particular measurement',\n",
       " 'total energy',\n",
       " 'basic approach',\n",
       " 'other sensor',\n",
       " 'individual sensor',\n",
       " 'local decision',\n",
       " 'local decision',\n",
       " 'additive white',\n",
       " 'square distribution',\n",
       " 'false target',\n",
       " 'false alarm',\n",
       " 'square distribution',\n",
       " 'square distribution',\n",
       " 'false target',\n",
       " 'above equation',\n",
       " 'analytic basis',\n",
       " 'false alarm',\n",
       " 'such algorithm',\n",
       " 'unauthorized activity',\n",
       " 'unauthorized traversal',\n",
       " 'stochastic characterization',\n",
       " 'false alarm',\n",
       " 'east periphery',\n",
       " 'net probability',\n",
       " 'east periphery',\n",
       " 'fixed frequency',\n",
       " 'constant speed',\n",
       " 'net probability',\n",
       " 'net probability',\n",
       " 'fine grid',\n",
       " 'rectangular grid',\n",
       " 'rectangular grid',\n",
       " 'rectangular grid',\n",
       " 'adjacent grid',\n",
       " 'virtual point',\n",
       " 'virtual point',\n",
       " 'total weight',\n",
       " 'adjacent grid',\n",
       " 'adjacent point',\n",
       " 'negative weight',\n",
       " 'fictitious point',\n",
       " 'fictitious point',\n",
       " 'east periphery',\n",
       " 'west periphery',\n",
       " 'east periphery',\n",
       " 'overall algorithm',\n",
       " 'dark circle',\n",
       " 'additive white',\n",
       " 'false alarm',\n",
       " 'weighted grid',\n",
       " 'traversal detection',\n",
       " 'unauthorized traversal',\n",
       " 'random fashion',\n",
       " 'traversal monitoring',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'false alarm',\n",
       " 'poor detection',\n",
       " 'same vicinity',\n",
       " 'statistical distribution',\n",
       " 'practical situation',\n",
       " 'limited number',\n",
       " 'limited detection',\n",
       " 'false alarm',\n",
       " 'above discussion',\n",
       " 'achievable detection',\n",
       " 'available sensor',\n",
       " 'random sensor',\n",
       " 'additional sensor',\n",
       " 'available sensor',\n",
       " 'single sensor',\n",
       " 'single sensor',\n",
       " 'optimal number',\n",
       " 'next section',\n",
       " 'analytical expression',\n",
       " 'optimal solution',\n",
       " 'optimal cost',\n",
       " 'analytical solution',\n",
       " 'analytical model',\n",
       " 'minimum exposure',\n",
       " 'random variable',\n",
       " 'cumulative distribution',\n",
       " 'previous section',\n",
       " 'maximum number',\n",
       " 'additional sensor',\n",
       " 'minimum exposure',\n",
       " 'additional sensor',\n",
       " 'few sensor',\n",
       " 'empty space',\n",
       " 'loose relation',\n",
       " 'different deployment',\n",
       " 'additional sensor',\n",
       " 'empty space',\n",
       " 'additional sensor',\n",
       " 'minimum exposure',\n",
       " 'random variable',\n",
       " 'variable c',\n",
       " 'total number',\n",
       " 'maximum number',\n",
       " 'first step',\n",
       " 'minimum exposure',\n",
       " 'possible event',\n",
       " 'different expression',\n",
       " 'additional sensor',\n",
       " 'predefined region',\n",
       " 'random deployment',\n",
       " 'local approximation',\n",
       " 'minimum exposure',\n",
       " 'common decision',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'different cost',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'constant density',\n",
       " 'small value',\n",
       " 'minimum exposure',\n",
       " 'large peak',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'poor confidence',\n",
       " 'desirable exposure',\n",
       " 'good exposure',\n",
       " 'cumulative distribution',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'optimal number',\n",
       " 'different cost',\n",
       " 'corresponding graph',\n",
       " 'first cost',\n",
       " 'expected number',\n",
       " 'minimum exposure',\n",
       " 'local minimum',\n",
       " 'following analysis',\n",
       " 'weighted sum',\n",
       " 'different number',\n",
       " 'minimum exposure',\n",
       " 'first term',\n",
       " 'local minimum',\n",
       " 'second cost',\n",
       " 'relative cost',\n",
       " 'optimal number',\n",
       " 'local minimum',\n",
       " 'last cost',\n",
       " 'minimum cost',\n",
       " 'similar result',\n",
       " 'other parameter',\n",
       " 'minimum exposure',\n",
       " 'sequential deployment',\n",
       " 'limited number',\n",
       " 'minimum exposure',\n",
       " 'random deployment',\n",
       " 'optimal number',\n",
       " 'relative cost',\n",
       " 'different target',\n",
       " 'variable number',\n",
       " 'multiple variable',\n",
       " 'further investigation']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_candidates['C-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# * Implement score function\n",
    "# * Use some other dataset instead of SemEval\n",
    "# * Debug the scores for tfidf\n",
    "\n",
    "from math import log\n",
    "def score(t, d, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    :param t: term\n",
    "    :param d: document-id in test dataset\n",
    "    \n",
    "    ftd = f(t, d): term frequency\n",
    "    avgdl = mean([len(doc) for doc in train])\n",
    "    N = len(train)\n",
    "    nt = n(t) = sum(1 for doc in train if t in doc)\n",
    "    \"\"\"\n",
    "    N = len(train_sentences)\n",
    "    nt = sum(1 for doc in train_frequencies if t in doc)\n",
    "    avgdl = np.mean([sum(frequencies.values()) for frequencies in test_frequencies.values()])\n",
    "    ftd = 1. * test_frequencies[d][t] / test_frequencies[d].most_common(1)[0][1]\n",
    "    ld = sum(test_frequencies[d].values())\n",
    "    \n",
    "    tf = (ftd * (k1 + 1)) / (ftd + k1 * (1 - b + b * ld / avgdl))\n",
    "    idf = log((N - nt + 0.5) / (nt + 0.5))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839753401748323"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score('active learning', 'H-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyphrases(doc_id, nb_keywords=5):\n",
    "    scores = {candidate: score(candidate, doc_id) for candidate in test_candidates[doc_id]}\n",
    "    scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:nb_keywords]\n",
    "    return [keyphrase for keyphrase, score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0754297cc7427cab9a072223bab664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for doc_id, doc in tqdm(test_sentences.items()):\n",
    "    keyphrases = extract_keyphrases(doc_id, nb_keywords=5)\n",
    "    predictions[doc_id] = keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['query term',\n",
       "  'excite query',\n",
       "  'web search',\n",
       "  'significant term',\n",
       "  'average time'],\n",
       " ['web summari', 'snippet gener', 'document cach'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['H-12'], target['H-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-1   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-14  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-17  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-18  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-19  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-20  -> Precision: 0.20 Recall: 0.33 F1: 0.25 precision@5: 0.20\n",
      "C-22  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-23  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-27  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-28  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-29  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-3   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-30  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-31  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-32  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-33  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-34  -> Precision: 0.20 Recall: 0.20 F1: 0.20 precision@5: 0.20\n",
      "C-36  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-38  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-4   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-40  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-6   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-8   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-86  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "C-9   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-10  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-11  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-12  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-13  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-14  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-16  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-17  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-18  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-19  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-2   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-20  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-21  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-24  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-25  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-26  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-29  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-3   -> Precision: 0.20 Recall: 0.25 F1: 0.22 precision@5: 0.20\n",
      "H-30  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-31  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-32  -> Precision: 0.20 Recall: 0.50 F1: 0.29 precision@5: 0.20\n",
      "H-4   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-5   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-7   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-8   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "H-9   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-1   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-10  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-11  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-12  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-14  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-15  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-16  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-18  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-19  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-20  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-21  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-22  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-26  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-29  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-30  -> Precision: 0.20 Recall: 0.20 F1: 0.20 precision@5: 0.20\n",
      "I-31  -> Precision: 0.20 Recall: 0.33 F1: 0.25 precision@5: 0.20\n",
      "I-32  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-33  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-34  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-35  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-4   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-5   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-6   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-7   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "I-9   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-1   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-10  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-11  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-13  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-14  -> Precision: 0.20 Recall: 0.33 F1: 0.25 precision@5: 0.20\n",
      "J-15  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-17  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-18  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-2   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-20  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-21  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-22  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-23  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-25  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-26  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-27  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-28  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-3   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-30  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-31  -> Precision: 0.20 Recall: 0.14 F1: 0.17 precision@5: 0.00\n",
      "J-32  -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-4   -> Precision: 0.20 Recall: 0.33 F1: 0.25 precision@5: 0.20\n",
      "J-7   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "J-8   -> Precision: 0.20 Recall: 0.12 F1: 0.15 precision@5: 0.20\n",
      "J-9   -> Precision: 0.00 Recall: 0.00 F1: 0.00 precision@5: 0.00\n",
      "\n",
      "--------------Mean-------------\n",
      "Precision: 0.02 Recall: 0.03 F1: 0.02   precision@5: 0.02\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, precision_5 = [], [], [], []\n",
    "for doc_id in sorted(predictions.keys()):\n",
    "    p = set(predictions[doc_id])\n",
    "    t = set(target[doc_id])\n",
    "    at_5 = set(target[doc_id][:5])\n",
    "\n",
    "    # We always predict 5 keywords\n",
    "    precision.append(len(p.intersection(t)) / len(p))\n",
    "    recall.append(len(p.intersection(t)) / len(t))\n",
    "    f1.append(0 if precision[-1] + recall[-1] == 0 else 2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n",
    "    precision_5.append(len(p.intersection(at_5)) / len(p))\n",
    "    print(f'{doc_id:5} -> Precision: {precision[-1]:.2f} Recall: {recall[-1]:.2f} F1: {f1[-1]:.2f} precision@5: {precision_5[-1]:.2f}')\n",
    "\n",
    "print()\n",
    "print('--------------Mean-------------')\n",
    "print(f'Precision: {np.mean(precision):.2f} Recall: {np.mean(recall):.2f} F1: {np.mean(f1):.2f}   precision@5: {np.mean(precision_5):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
