{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyPhrase extraction\n",
    "* Use SemEval 2010 dataset - [train](https://github.com/boudinfl/ake-datasets/blob/master/datasets/SemEval-2010/train/) dataset for TF-IDF vectorization\n",
    "* Use SemEval 2010 dataset - [test](https://github.com/boudinfl/ake-datasets/blob/master/datasets/SemEval-2010/test/) for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from nltk import ngrams\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xml.etree import ElementTree\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sno = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(directory):\n",
    "    docs = {}\n",
    "    for doc_path in tqdm(glob(f'{directory}/*.xml')):\n",
    "        doc = ElementTree.parse(doc_path)\n",
    "        sentences = []\n",
    "        for sentence in doc.find('document').find('sentences').findall('sentence'):\n",
    "            sentences.append(' '.join([token.find('lemma').text.lower() + '~' + token.find('POS').text\n",
    "                                       for token in sentence.find('tokens').findall('token')]))\n",
    "\n",
    "        docs[doc_path.split('/')[-1].split('.')[0]] = '\\n'.join(sentences)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6f9633dcfe4f8794b34c85e235e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=144), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a69573bb12423187b20169e132c2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read('ake-datasets/datasets/SemEval-2010/train')\n",
    "test_sentences = read('ake-datasets/datasets/SemEval-2010/test')\n",
    "len(train_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(((\\w+~JJ)* (\\w+~NN)+ (\\w+~IN))?(\\w+~JJ)+ (\\w+~NN)+)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_candidates = {doc_id: [candidate[0] for candidate in re.findall(pattern, doc)] for doc_id, doc in train_sentences.items()}\n",
    "train_candidates = {doc_id: [' '.join([w.split('~')[0] for w in candidate.split()]) for candidate in candidates] for doc_id, candidates in train_candidates.items()}\n",
    "train_sentences = {doc_id: ' '.join([w.split('~')[0] for w in sentences.split()]) for doc_id, sentences in train_sentences.items()}\n",
    "train_frequencies = {doc_id: Counter(\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 1)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 2)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 3)])\n",
    "                    for doc_id, doc in train_sentences.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates = {doc_id: [candidate[0] for candidate in re.findall(pattern, doc)] for doc_id, doc in test_sentences.items()}\n",
    "test_candidates = {doc_id: [' '.join([w.split('~')[0] for w in candidate.split()]) for candidate in candidates] for doc_id, candidates in test_candidates.items()}\n",
    "test_sentences = {doc_id: ' '.join([w.split('~')[0] for w in sentences.split()]) for doc_id, sentences in test_sentences.items()}\n",
    "test_frequencies = {doc_id: Counter(\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 1)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 2)] + \\\n",
    "                                [' '.join(gram) for gram in ngrams(doc.split(), 3)])\n",
    "                    for doc_id, doc in test_sentences.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 3))\n",
    "trainvec = vectorizer.fit_transform(train_sentences.values())\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ake-datasets/datasets/SemEval-2010/references/test.author.stem.json', 'r') as f:\n",
    "    target = json.load(f)\n",
    "    target = {doc_name: [k[0] for k in keyphrases] for doc_name, keyphrases in target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collabor target detect',\n",
       " 'deploy',\n",
       " 'exposur',\n",
       " 'sensor network',\n",
       " 'valu fusion']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['C-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collaborative target',\n",
       " 'certain level',\n",
       " 'sequential deployment',\n",
       " 'recent advance',\n",
       " 'specific event',\n",
       " 'specific region',\n",
       " 'such system',\n",
       " 'civil infrastructure',\n",
       " 'other means',\n",
       " 'random placement',\n",
       " 'local observation',\n",
       " 'global decision',\n",
       " 'local processing',\n",
       " 'different node',\n",
       " 'local observation',\n",
       " 'possible measure',\n",
       " 'practical application',\n",
       " 'precise placement',\n",
       " 'random deployment',\n",
       " 'several solution',\n",
       " 'analytical study',\n",
       " 'optimum solution',\n",
       " 'rectangular sensor',\n",
       " 'geometric distance',\n",
       " 'particular measurement',\n",
       " 'total energy',\n",
       " 'basic approach',\n",
       " 'other sensor',\n",
       " 'individual sensor',\n",
       " 'local decision',\n",
       " 'local decision',\n",
       " 'additive white',\n",
       " 'square distribution',\n",
       " 'false target',\n",
       " 'false alarm',\n",
       " 'square distribution',\n",
       " 'square distribution',\n",
       " 'false target',\n",
       " 'above equation',\n",
       " 'analytic basis',\n",
       " 'false alarm',\n",
       " 'such algorithm',\n",
       " 'unauthorized activity',\n",
       " 'unauthorized traversal',\n",
       " 'stochastic characterization',\n",
       " 'false alarm',\n",
       " 'east periphery',\n",
       " 'net probability',\n",
       " 'east periphery',\n",
       " 'fixed frequency',\n",
       " 'constant speed',\n",
       " 'net probability',\n",
       " 'net probability',\n",
       " 'fine grid',\n",
       " 'rectangular grid',\n",
       " 'rectangular grid',\n",
       " 'rectangular grid',\n",
       " 'adjacent grid',\n",
       " 'virtual point',\n",
       " 'virtual point',\n",
       " 'total weight',\n",
       " 'adjacent grid',\n",
       " 'adjacent point',\n",
       " 'negative weight',\n",
       " 'fictitious point',\n",
       " 'fictitious point',\n",
       " 'east periphery',\n",
       " 'west periphery',\n",
       " 'east periphery',\n",
       " 'overall algorithm',\n",
       " 'dark circle',\n",
       " 'additive white',\n",
       " 'false alarm',\n",
       " 'weighted grid',\n",
       " 'traversal detection',\n",
       " 'unauthorized traversal',\n",
       " 'random fashion',\n",
       " 'traversal monitoring',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'false alarm',\n",
       " 'poor detection',\n",
       " 'same vicinity',\n",
       " 'statistical distribution',\n",
       " 'practical situation',\n",
       " 'limited number',\n",
       " 'limited detection',\n",
       " 'false alarm',\n",
       " 'above discussion',\n",
       " 'achievable detection',\n",
       " 'available sensor',\n",
       " 'random sensor',\n",
       " 'additional sensor',\n",
       " 'available sensor',\n",
       " 'single sensor',\n",
       " 'single sensor',\n",
       " 'optimal number',\n",
       " 'next section',\n",
       " 'analytical expression',\n",
       " 'optimal solution',\n",
       " 'optimal cost',\n",
       " 'analytical solution',\n",
       " 'analytical model',\n",
       " 'minimum exposure',\n",
       " 'random variable',\n",
       " 'cumulative distribution',\n",
       " 'previous section',\n",
       " 'maximum number',\n",
       " 'additional sensor',\n",
       " 'minimum exposure',\n",
       " 'additional sensor',\n",
       " 'few sensor',\n",
       " 'empty space',\n",
       " 'loose relation',\n",
       " 'different deployment',\n",
       " 'additional sensor',\n",
       " 'empty space',\n",
       " 'additional sensor',\n",
       " 'minimum exposure',\n",
       " 'random variable',\n",
       " 'variable c',\n",
       " 'total number',\n",
       " 'maximum number',\n",
       " 'first step',\n",
       " 'minimum exposure',\n",
       " 'possible event',\n",
       " 'different expression',\n",
       " 'additional sensor',\n",
       " 'predefined region',\n",
       " 'random deployment',\n",
       " 'local approximation',\n",
       " 'minimum exposure',\n",
       " 'common decision',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'different cost',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'false alarm',\n",
       " 'minimum exposure',\n",
       " 'constant density',\n",
       " 'small value',\n",
       " 'minimum exposure',\n",
       " 'large peak',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'poor confidence',\n",
       " 'desirable exposure',\n",
       " 'good exposure',\n",
       " 'cumulative distribution',\n",
       " 'minimum exposure',\n",
       " 'minimum exposure',\n",
       " 'optimal number',\n",
       " 'different cost',\n",
       " 'corresponding graph',\n",
       " 'first cost',\n",
       " 'expected number',\n",
       " 'minimum exposure',\n",
       " 'local minimum',\n",
       " 'following analysis',\n",
       " 'weighted sum',\n",
       " 'different number',\n",
       " 'minimum exposure',\n",
       " 'first term',\n",
       " 'local minimum',\n",
       " 'second cost',\n",
       " 'relative cost',\n",
       " 'optimal number',\n",
       " 'local minimum',\n",
       " 'last cost',\n",
       " 'minimum cost',\n",
       " 'similar result',\n",
       " 'other parameter',\n",
       " 'minimum exposure',\n",
       " 'sequential deployment',\n",
       " 'limited number',\n",
       " 'minimum exposure',\n",
       " 'random deployment',\n",
       " 'optimal number',\n",
       " 'relative cost',\n",
       " 'different target',\n",
       " 'variable number',\n",
       " 'multiple variable',\n",
       " 'further investigation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_candidates['C-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# * Implement score function\n",
    "# * Use some other dataset instead of SemEval\n",
    "# * Debug the scores for tfidf\n",
    "\n",
    "from math import log\n",
    "def score(t, d, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    :param t: term\n",
    "    :param d: document-id in test dataset\n",
    "    \n",
    "    ftd = f(t, d): term frequency\n",
    "    avgdl = mean([len(doc) for doc in train])\n",
    "    N = len(train)\n",
    "    nt = n(t) = sum(1 for doc in train if t in doc)\n",
    "    \"\"\"\n",
    "    N = len(train_sentences)\n",
    "    nt = sum(1 for doc in train_frequencies if t in doc)\n",
    "    avgdl = np.mean([sum(frequencies.values()) for frequencies in test_frequencies.values()])\n",
    "    ftd = 1. * test_frequencies[d][t] / test_frequencies[d].most_common(1)[0][1]\n",
    "    ld = sum(test_frequencies[d].values())\n",
    "    \n",
    "    tf = (ftd * (k1 + 1)) / (ftd + k1 * (1 - b + b * ld / avgdl))\n",
    "    idf = log((N - nt + 0.5) / (nt + 0.5))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839753401748323"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score('active learning', 'H-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyphrases(doc_id, nb_keywords=5):\n",
    "    scores = {candidate: score(candidate, doc_id) for candidate in test_candidates[doc_id]}\n",
    "    scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:nb_keywords]\n",
    "    return [keyphrase for keyphrase, score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59254113da74381bf6f8f579901858e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = {}\n",
    "for doc_id, doc in tqdm(test_sentences.items()):\n",
    "    keyphrases = extract_keyphrases(doc_id, nb_keywords=5)\n",
    "    predictions[doc_id] = keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {doc_id: [sno.stem(candidate) for candidate in candidates] for doc_id, candidates in predictions.items()}\n",
    "target = {doc_id: [sno.stem(candidate) for candidate in candidates] for doc_id, candidates in target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['C-34'], target['C-34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, precision_5 = [], [], [], []\n",
    "for doc_id in sorted(predictions.keys()):\n",
    "    p = set(predictions[doc_id])\n",
    "    t = set(target[doc_id])\n",
    "    at_5 = set(target[doc_id][:5])\n",
    "\n",
    "    # We always predict 5 keywords\n",
    "    precision.append(len(p.intersection(t)) / len(p))\n",
    "    recall.append(len(p.intersection(t)) / len(t))\n",
    "    f1.append(0 if precision[-1] + recall[-1] == 0 else 2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n",
    "    precision_5.append(len(p.intersection(at_5)) / len(p))\n",
    "    print(f'{doc_id:5} -> Precision: {precision[-1]:.2f} Recall: {recall[-1]:.2f} F1: {f1[-1]:.2f} precision@5: {precision_5[-1]:.2f}')\n",
    "\n",
    "print()\n",
    "print('--------------Mean-------------')\n",
    "print(f'Precision: {np.mean(precision):.2f} Recall: {np.mean(recall):.2f} F1: {np.mean(f1):.2f}   precision@5: {np.mean(precision_5):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
