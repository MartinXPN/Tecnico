{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import json\n",
    "import itertools\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk import everygrams\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xml.etree import ElementTree\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sno = SnowballStemmer('english')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c456a45503d748d0aeb67f03b9a675e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd13645b9f5a4beaa92cc7df1a2f1e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read(directory):\n",
    "    docs = {}\n",
    "    for doc_path in tqdm(glob(f'{directory}/*.xml')):\n",
    "        doc = ElementTree.parse(doc_path)\n",
    "        sentences = []\n",
    "        for sentence in doc.find('document').find('sentences').findall('sentence'):\n",
    "            sentences.append(' '.join([token.find('lemma').text.lower() \n",
    "                                       for token in sentence.find('tokens').findall('token')]))\n",
    "\n",
    "        docs[doc_path.split('/')[-1].split('.')[0]] = '\\n'.join(sentences)\n",
    "    return docs\n",
    "\n",
    "\n",
    "train_docs = read('../Project1/ake-datasets/datasets/Inspec/train')\n",
    "test_docs = read('../Project1/ake-datasets/datasets/Inspec/test')\n",
    "len(train_docs), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['out-of-print materials',\n",
       " 'recurring issues',\n",
       " 'changing practices',\n",
       " 'out-of-print books',\n",
       " 'library materials',\n",
       " 'acquisition']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../Project1/ake-datasets/datasets/Inspec/references/test.uncontr.json', 'r') as f:\n",
    "    target = json.load(f)\n",
    "    target = {doc_name: [k[0] for k in keyphrases] for doc_name, keyphrases in target.items()}\n",
    "target['193']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 3))\n",
    "trainvec = vectorizer.fit_transform(train_docs.values())\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(vec, feature_names):\n",
    "    feature_index = vec.nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [vec[0, x] for x in feature_index])\n",
    "    tfidf_scores = {feature_names[i]: s for i, s in tfidf_scores}\n",
    "    return tfidf_scores\n",
    "\n",
    "# tf_idf(test_sentences['193'], vectorizer, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precisoin(pred, targ):\n",
    "    pred = set([sno.stem(p) for p in pred])\n",
    "    targ = set([sno.stem(t) for t in targ])\n",
    "    res, nb_correct = 0, 0\n",
    "    for i, p in enumerate(pred):\n",
    "        if p in targ:\n",
    "            nb_correct += 1\n",
    "        res += nb_correct / (i + 1)\n",
    "    return 1. / len(targ) * res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = vectorizer.transform(test_docs.values())\n",
    "test_docs = {doc_id: (test_docs[doc_id], vec) for doc_id, vec in zip(test_docs, test_vecs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(doc_id, doc, get_weight, get_personalization=None, weight='weight'):\n",
    "    G = nx.Graph(name=doc_id)\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        tokens = [str(t) for t in sentence if t.is_alpha and not t.is_stop]\n",
    "        grams = everygrams(tokens, min_len=1, max_len=3)\n",
    "        grams = [' '.join(g) for g in grams]\n",
    "        G.add_nodes_from(grams)\n",
    "        \n",
    "        edges = list(itertools.combinations(grams, 2))\n",
    "        weighted_edges = [(v1, v2, get_weight(v1, v2)) for v1, v2 in edges]\n",
    "        G.add_weighted_edges_from(weighted_edges)\n",
    "    \n",
    "    personalization = {node: get_personalization(node) for node in G.nodes} if get_personalization else None\n",
    "    rank = nx.pagerank(G, alpha=1-0.15, max_iter=50, weight=weight, personalization=personalization)\n",
    "    top = sorted(rank.items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    top_keywords = [k for k, s in top]\n",
    "    return avg_precisoin(pred=top_keywords, targ=target[doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379b828dc9e24dfa9a75e045a7c00bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.040379148926695294\n"
     ]
    }
   ],
   "source": [
    "precision = [score(doc_id, doc, get_weight=lambda x, y: 1, weight=None)\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF scores as personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe02d9ad55843549c83b777947e4c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.04160085789000417\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):    \n",
    "    tfidf_scores = tf_idf(vec, feature_names)\n",
    "    \n",
    "    s = score(doc_id, doc, get_weight=lambda x, y: 1,\n",
    "              get_personalization=lambda x: tfidf_scores[x] if x in tfidf_scores else 0)\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position score as personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d93f0d37546b78b464a7f2c00752b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.05410335629573787\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):   \n",
    "    d = nlp(doc)\n",
    "    d = ' '.join([str(t) for t in d if t.is_alpha and not t.is_stop])\n",
    "    s = score(doc_id, doc, get_weight=lambda x, y: 1,\n",
    "              get_personalization=lambda x: 1 / (1 + log(1 + d.find(x))))\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd6e28696a4947bc6bdd18c43a3524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "W = Counter()\n",
    "for doc_id, doc in tqdm(train_docs.items()):\n",
    "    doc = nlp(doc)\n",
    "    for sentence in doc.sents:\n",
    "        tokens = [str(t) for t in sentence if t.is_alpha and not t.is_stop]\n",
    "        grams = everygrams(tokens, min_len=1, max_len=3)\n",
    "        grams = [' '.join(g) for g in grams]\n",
    "        for w1, w2 in itertools.combinations(grams, 2):\n",
    "            W[(w1, w2)] += 1\n",
    "            W[(w2, w1)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf14d10afe4b4fb58a937d9f496ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.003976034521034521\n"
     ]
    }
   ],
   "source": [
    "precision = [score(doc_id, doc, get_weight=lambda x, y: W[(x, y)])\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fasttext import load_model\n",
    "model = load_model('./cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e98153a43e4584bf16f39cdaeea91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.05591094568199261\n"
     ]
    }
   ],
   "source": [
    "def weight(w1, w2):\n",
    "    v1 = model.get_sentence_vector(w1)\n",
    "    v2 = model.get_sentence_vector(w2)\n",
    "    return cosine_similarity([v1], [v2])[0]\n",
    "\n",
    "precision = [score(doc_id, doc, get_weight=weight)\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf0e02d7faa434d933273e21409f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.061146989911527554\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):   \n",
    "    d = nlp(doc)\n",
    "    d = ' '.join([str(t) for t in d if t.is_alpha and not t.is_stop])\n",
    "    s = score(doc_id, doc, get_weight=weight,\n",
    "              get_personalization=lambda x: 1 / (1 + log(1 + d.find(x))))\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
