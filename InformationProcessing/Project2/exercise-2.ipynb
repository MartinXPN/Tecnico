{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import json\n",
    "import itertools\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk import everygrams\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xml.etree import ElementTree\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sno = SnowballStemmer('english')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffb94c9075643b4a1c121e48049ae2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9639a809083c425681e7025ca584bc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read(directory):\n",
    "    docs = {}\n",
    "    for doc_path in tqdm(glob(f'{directory}/*.xml')):\n",
    "        doc = ElementTree.parse(doc_path)\n",
    "        sentences = []\n",
    "        for sentence in doc.find('document').find('sentences').findall('sentence'):\n",
    "            sentences.append(' '.join([token.find('lemma').text.lower() \n",
    "                                       for token in sentence.find('tokens').findall('token')]))\n",
    "\n",
    "        docs[doc_path.split('/')[-1].split('.')[0]] = '\\n'.join(sentences)\n",
    "    return docs\n",
    "\n",
    "\n",
    "train_docs = read('../Project1/ake-datasets/datasets/Inspec/train')\n",
    "test_docs = read('../Project1/ake-datasets/datasets/Inspec/test')\n",
    "len(train_docs), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['out-of-print materials',\n",
       " 'recurring issues',\n",
       " 'changing practices',\n",
       " 'out-of-print books',\n",
       " 'library materials',\n",
       " 'acquisition']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../Project1/ake-datasets/datasets/Inspec/references/test.uncontr.json', 'r') as f:\n",
    "    target = json.load(f)\n",
    "    target = {doc_name: [k[0] for k in keyphrases] for doc_name, keyphrases in target.items()}\n",
    "target['193']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 3))\n",
    "trainvec = vectorizer.fit_transform(train_docs.values())\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(vec, feature_names):\n",
    "    feature_index = vec.nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [vec[0, x] for x in feature_index])\n",
    "    tfidf_scores = {feature_names[i]: s for i, s in tfidf_scores}\n",
    "    return tfidf_scores\n",
    "\n",
    "# tf_idf(test_sentences['193'], vectorizer, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precisoin(pred, targ):\n",
    "    pred = set([sno.stem(p) for p in pred])\n",
    "    targ = set([sno.stem(t) for t in targ])\n",
    "    res, nb_correct = 0, 0\n",
    "    for i, p in enumerate(pred):\n",
    "        if p in targ:\n",
    "            nb_correct += 1\n",
    "            res += nb_correct / (i + 1)\n",
    "    return res / len(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = vectorizer.transform(test_docs.values())\n",
    "test_docs = {doc_id: (test_docs[doc_id], vec) for doc_id, vec in zip(test_docs, test_vecs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(doc_id, doc, get_weight, get_personalization=None, weight='weight'):\n",
    "    G = nx.Graph(name=doc_id)\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        tokens = [str(t) for t in sentence if t.is_alpha and not t.is_stop]\n",
    "        grams = everygrams(tokens, min_len=1, max_len=3)\n",
    "        grams = [' '.join(g) for g in grams]\n",
    "        G.add_nodes_from(grams)\n",
    "        \n",
    "        edges = list(itertools.combinations(grams, 2))\n",
    "        weighted_edges = [(v1, v2, get_weight(v1, v2)) for v1, v2 in edges]\n",
    "        G.add_weighted_edges_from(weighted_edges)\n",
    "    \n",
    "    personalization = {node: get_personalization(node) for node in G.nodes} if get_personalization else None\n",
    "    rank = nx.pagerank(G, alpha=1-0.15, max_iter=50, weight=weight, personalization=personalization)\n",
    "    top = sorted(rank.items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    top_keywords = [k for k, s in top]\n",
    "    return avg_precisoin(pred=top_keywords, targ=target[doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01930bdd9f44d94b34aacdad37ff79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.020347820170048946\n"
     ]
    }
   ],
   "source": [
    "precision = [score(doc_id, doc, get_weight=lambda x, y: 1, weight=None)\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF scores as personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe20e70b566479aa9d0a0f210b988b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.01931806907657972\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):    \n",
    "    tfidf_scores = tf_idf(vec, feature_names)\n",
    "    \n",
    "    s = score(doc_id, doc, get_weight=lambda x, y: 1,\n",
    "              get_personalization=lambda x: tfidf_scores[x] if x in tfidf_scores else 0)\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position score as personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3da6afe6c24f9187a5b68a7c363a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.025928934001415233\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):   \n",
    "    d = nlp(doc)\n",
    "    d = ' '.join([str(t) for t in d if t.is_alpha and not t.is_stop])\n",
    "    s = score(doc_id, doc, get_weight=lambda x, y: 1,\n",
    "              get_personalization=lambda x: 1 / (1 + log(1 + d.find(x))))\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d221f02596454ca082e4bd85b5637822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "W = Counter()\n",
    "for doc_id, doc in tqdm(train_docs.items()):\n",
    "    doc = nlp(doc)\n",
    "    for sentence in doc.sents:\n",
    "        tokens = [str(t) for t in sentence if t.is_alpha and not t.is_stop]\n",
    "        grams = everygrams(tokens, min_len=1, max_len=3)\n",
    "        grams = [' '.join(g) for g in grams]\n",
    "        for w1, w2 in itertools.combinations(grams, 2):\n",
    "            W[(w1, w2)] += 1\n",
    "            W[(w2, w1)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6212aad01248789f3ad3b616badbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0033891428016428016\n"
     ]
    }
   ],
   "source": [
    "precision = [score(doc_id, doc, get_weight=lambda x, y: W[(x, y)])\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fasttext import load_model\n",
    "model = load_model('./cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4248adaa32ca49f09472d4c203100ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.026529322324896437\n"
     ]
    }
   ],
   "source": [
    "def weight(w1, w2):\n",
    "    v1 = model.get_sentence_vector(w1)\n",
    "    v2 = model.get_sentence_vector(w2)\n",
    "    return cosine_similarity([v1], [v2])[0]\n",
    "\n",
    "precision = [score(doc_id, doc, get_weight=weight)\n",
    "             for doc_id, (doc, vec) in tqdm(test_docs.items())]\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b287c2012f47c3a1033bd6efb6125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.03193933865403732\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "\n",
    "for doc_id, (doc, vec) in tqdm(test_docs.items()):   \n",
    "    d = nlp(doc)\n",
    "    d = ' '.join([str(t) for t in d if t.is_alpha and not t.is_stop])\n",
    "    s = score(doc_id, doc, get_weight=weight,\n",
    "              get_personalization=lambda x: 1 / (1 + log(1 + d.find(x))))\n",
    "    precision.append(s)\n",
    "\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
