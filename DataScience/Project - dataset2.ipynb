{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "%matplotlib inline\n",
    "import ipynb.fs.defs.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register_matplotlib_converters()\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and transform the format for easier manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y13</th>\n",
       "      <th>y14</th>\n",
       "      <th>y15</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y18</th>\n",
       "      <th>y19</th>\n",
       "      <th>y20</th>\n",
       "      <th>y21</th>\n",
       "      <th>y22</th>\n",
       "      <th>y23</th>\n",
       "      <th>y24</th>\n",
       "      <th>y25</th>\n",
       "      <th>y26</th>\n",
       "      <th>y27</th>\n",
       "      <th>y28</th>\n",
       "      <th>y29</th>\n",
       "      <th>y30</th>\n",
       "      <th>y31</th>\n",
       "      <th>y32</th>\n",
       "      <th>y33</th>\n",
       "      <th>y34</th>\n",
       "      <th>y35</th>\n",
       "      <th>y36</th>\n",
       "      <th>y37</th>\n",
       "      <th>y38</th>\n",
       "      <th>y39</th>\n",
       "      <th>y40</th>\n",
       "      <th>y41</th>\n",
       "      <th>y42</th>\n",
       "      <th>y43</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points  y0  y1  y2  y3  y4  y5  y6  y7  y8  y9  y10  y11  y12  y13  y14  y15  y16  y17  y18  y19  y20  y21  y22  y23  y24  y25  y26  y27  y28  y29  y30  y31  y32  y33  y34  y35  y36  y37  y38  y39  y40  y41  y42  y43  Cover_Type\n",
       "0       2596      51      3                               258                               0                              510            221             232            148                                6279   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0           5\n",
       "1       2590      56      2                               212                              -6                              390            220             235            151                                6225   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0           5\n",
       "2       2804     139      9                               268                              65                             3180            234             238            135                                6121   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0           2\n",
       "3       2785     155     18                               242                             118                             3090            238             238            122                                6211   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0           2\n",
       "4       2595      45      2                               153                              -1                              391            220             234            150                                6172   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0           5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('covtype.data', sep=\",\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y13</th>\n",
       "      <th>y14</th>\n",
       "      <th>y15</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y18</th>\n",
       "      <th>y19</th>\n",
       "      <th>y20</th>\n",
       "      <th>y21</th>\n",
       "      <th>y22</th>\n",
       "      <th>y23</th>\n",
       "      <th>y24</th>\n",
       "      <th>y25</th>\n",
       "      <th>y26</th>\n",
       "      <th>y27</th>\n",
       "      <th>y28</th>\n",
       "      <th>y29</th>\n",
       "      <th>y30</th>\n",
       "      <th>y31</th>\n",
       "      <th>y32</th>\n",
       "      <th>y33</th>\n",
       "      <th>y34</th>\n",
       "      <th>y35</th>\n",
       "      <th>y36</th>\n",
       "      <th>y37</th>\n",
       "      <th>y38</th>\n",
       "      <th>y39</th>\n",
       "      <th>y40</th>\n",
       "      <th>y41</th>\n",
       "      <th>y42</th>\n",
       "      <th>y43</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points  y0  y1  y2  y3  y4  y5  y6  y7  y8  y9  y10  y11  y12  y13  y14  y15  y16  y17  y18  y19  y20  y21  y22  y23  y24  y25  y26  y27  y28  y29  y30  y31  y32  y33  y34  y35  y36  y37  y38  y39  y40  y41  y42  y43  class\n",
       "0       2596      51      3                               258                               0                              510            221             232            148                                6279   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0      4\n",
       "1       2590      56      2                               212                              -6                              390            220             235            151                                6225   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0      4\n",
       "2       2804     139      9                               268                              65                             3180            234             238            135                                6121   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0      1\n",
       "3       2785     155     18                               242                             118                             3090            238             238            122                                6211   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0      1\n",
       "4       2595      45      2                               153                              -1                              391            220             234            150                                6172   1   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'] = data['Cover_Type'] - 1\n",
    "data.drop(columns=['Cover_Type'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some generic data exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the whole dataset!\n"
     ]
    }
   ],
   "source": [
    "# Top variables with missing values:\n",
    "mv = {}\n",
    "for var in data:\n",
    "    mv[var] = data[var].isna().sum()\n",
    "\n",
    "top_missing = sorted(mv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "if any([nb_missing for nb_missing in mv.values()]):\n",
    "    print(top_missing)\n",
    "else:\n",
    "    print('No missing values in the whole dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y13</th>\n",
       "      <th>y14</th>\n",
       "      <th>y15</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y18</th>\n",
       "      <th>y19</th>\n",
       "      <th>y20</th>\n",
       "      <th>y21</th>\n",
       "      <th>y22</th>\n",
       "      <th>y23</th>\n",
       "      <th>y24</th>\n",
       "      <th>y25</th>\n",
       "      <th>y26</th>\n",
       "      <th>y27</th>\n",
       "      <th>y28</th>\n",
       "      <th>y29</th>\n",
       "      <th>y30</th>\n",
       "      <th>y31</th>\n",
       "      <th>y32</th>\n",
       "      <th>y33</th>\n",
       "      <th>y34</th>\n",
       "      <th>y35</th>\n",
       "      <th>y36</th>\n",
       "      <th>y37</th>\n",
       "      <th>y38</th>\n",
       "      <th>y39</th>\n",
       "      <th>y40</th>\n",
       "      <th>y41</th>\n",
       "      <th>y42</th>\n",
       "      <th>y43</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2959.365301</td>\n",
       "      <td>155.656807</td>\n",
       "      <td>14.103704</td>\n",
       "      <td>269.428217</td>\n",
       "      <td>46.418855</td>\n",
       "      <td>2350.146611</td>\n",
       "      <td>212.146049</td>\n",
       "      <td>223.318716</td>\n",
       "      <td>142.528263</td>\n",
       "      <td>1980.291226</td>\n",
       "      <td>0.448865</td>\n",
       "      <td>0.051434</td>\n",
       "      <td>0.436074</td>\n",
       "      <td>0.063627</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.012952</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.056168</td>\n",
       "      <td>0.021359</td>\n",
       "      <td>0.051584</td>\n",
       "      <td>0.030001</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.057439</td>\n",
       "      <td>0.099399</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.198356</td>\n",
       "      <td>0.051927</td>\n",
       "      <td>0.044175</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>1.051471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>279.984734</td>\n",
       "      <td>111.913721</td>\n",
       "      <td>7.488242</td>\n",
       "      <td>212.549356</td>\n",
       "      <td>58.295232</td>\n",
       "      <td>1559.254870</td>\n",
       "      <td>26.769889</td>\n",
       "      <td>19.768697</td>\n",
       "      <td>38.274529</td>\n",
       "      <td>1324.195210</td>\n",
       "      <td>0.497379</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>0.495897</td>\n",
       "      <td>0.244087</td>\n",
       "      <td>0.072039</td>\n",
       "      <td>0.113066</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.144499</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.105775</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.044387</td>\n",
       "      <td>0.230245</td>\n",
       "      <td>0.144579</td>\n",
       "      <td>0.221186</td>\n",
       "      <td>0.170590</td>\n",
       "      <td>0.032092</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.069804</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.057077</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.125228</td>\n",
       "      <td>0.037950</td>\n",
       "      <td>0.232681</td>\n",
       "      <td>0.299197</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>0.028551</td>\n",
       "      <td>0.066605</td>\n",
       "      <td>0.043193</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.398762</td>\n",
       "      <td>0.221879</td>\n",
       "      <td>0.205483</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>1.396504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1859.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2809.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2996.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3163.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>3328.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3858.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>7117.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>7173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Elevation         Aspect          Slope  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points             y0             y1             y2             y3             y4             y5             y6             y7             y8             y9            y10            y11            y12            y13            y14            y15            y16            y17            y18            y19            y20            y21            y22            y23            y24            y25            y26            y27            y28            y29            y30            y31            y32            y33            y34            y35            y36            y37            y38            y39            y40            y41            y42            y43          class\n",
       "count  581012.000000  581012.000000  581012.000000                     581012.000000                   581012.000000                    581012.000000  581012.000000   581012.000000  581012.000000                       581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000\n",
       "mean     2959.365301     155.656807      14.103704                        269.428217                       46.418855                      2350.146611     212.146049      223.318716     142.528263                         1980.291226       0.448865       0.051434       0.436074       0.063627       0.005217       0.012952       0.008301       0.021335       0.002749       0.011316       0.000181       0.000308       0.001974       0.056168       0.021359       0.051584       0.030001       0.001031       0.000005       0.004897       0.005890       0.003268       0.006921       0.015936       0.001442       0.057439       0.099399       0.036622       0.000816       0.004456       0.001869       0.001628       0.198356       0.051927       0.044175       0.090392       0.077716       0.002773       0.003255       0.000205       0.000513       0.026803       0.023762       0.015060       1.051471\n",
       "std       279.984734     111.913721       7.488242                        212.549356                       58.295232                      1559.254870      26.769889       19.768697      38.274529                         1324.195210       0.497379       0.220882       0.495897       0.244087       0.072039       0.113066       0.090731       0.144499       0.052356       0.105775       0.013442       0.017550       0.044387       0.230245       0.144579       0.221186       0.170590       0.032092       0.002272       0.069804       0.076518       0.057077       0.082902       0.125228       0.037950       0.232681       0.299197       0.187833       0.028551       0.066605       0.043193       0.040318       0.398762       0.221879       0.205483       0.286743       0.267725       0.052584       0.056957       0.014310       0.022641       0.161508       0.152307       0.121791       1.396504\n",
       "min      1859.000000       0.000000       0.000000                          0.000000                     -173.000000                         0.000000       0.000000        0.000000       0.000000                            0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000\n",
       "25%      2809.000000      58.000000       9.000000                        108.000000                        7.000000                      1106.000000     198.000000      213.000000     119.000000                         1024.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000\n",
       "50%      2996.000000     127.000000      13.000000                        218.000000                       30.000000                      1997.000000     218.000000      226.000000     143.000000                         1710.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       1.000000\n",
       "75%      3163.000000     260.000000      18.000000                        384.000000                       69.000000                      3328.000000     231.000000      237.000000     168.000000                         2550.000000       1.000000       0.000000       1.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       1.000000\n",
       "max      3858.000000     360.000000      66.000000                       1397.000000                      601.000000                      7117.000000     254.000000      254.000000     254.000000                         7173.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       6.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very informative...\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    283301\n",
      "0    211840\n",
      "2     35754\n",
      "6     20510\n",
      "5     17367\n",
      "4      9493\n",
      "3      2747\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_count = pd.value_counts(data['class'].values.flatten())\n",
    "min_class = target_count.idxmin()\n",
    "ind_min_class = target_count.index.get_loc(min_class)\n",
    "plt.bar(target_count.index, target_count.values)\n",
    "print(target_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "As each preprocessing step has to be numerically analized and reasoned about, for each preprocessing technique we will evaluate the preprocessing technique on several classifiers and decide if we want to keep the change or not based on the performance gain.\n",
    "\n",
    "The performance will be evaluated on several classifiers including knn, naive bayes, xgboost, etc. If there was a performance gain after applying that preprocessing step, we will include that preprocessing step in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, test_size=0.3):\n",
    "    X = df.loc[:, ~df.columns.isin(['class'])]\n",
    "    y = df.loc[:, df.columns.isin(['class'])]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y.values.flatten(), test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df, split_dataset=split, knn=None, tree=None, rf=None, boost=None):\n",
    "    def report(y_pred):\n",
    "        return {\n",
    "            'Accuracy': metrics.accuracy_score(y_test, y_pred),\n",
    "#             'ROC AUC': metrics.roc_auc_score(y_test, y_pred),\n",
    "#             'Confusion Matrix': metrics.confusion_matrix(y_test, y_pred),\n",
    "            'Sensitivity': metrics.classification_report(y_test, y_pred, output_dict=True)['1']['recall'],\n",
    "        }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_dataset(df)\n",
    "    res = {}\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    res['Naive Bayes'] = report(nb.predict(X_test))\n",
    "    \n",
    "    if knn is None:\n",
    "        knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    res['KNN'] = report(knn.predict(X_test))\n",
    "    \n",
    "    if tree is None:\n",
    "        tree = DecisionTreeClassifier(class_weight='balanced')\n",
    "    tree.fit(X_train, y_train)\n",
    "    res['Decision Tree'] = report(tree.predict(X_test))\n",
    "\n",
    "    if rf is None:\n",
    "        rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "    rf.fit(X_train, y_train)\n",
    "    res['Random Forest'] = report(rf.predict(X_test))\n",
    "    \n",
    "    D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "    if boost is None:\n",
    "        boost = xgb.train({'eta': 0.3, 'max_depth': 5, 'objective': 'multi:softprob', 'num_class': len(target_count)}, D_train, 10)\n",
    "    preds = boost.predict(D_test)\n",
    "    res['xgboost'] = report(np.asarray([np.argmax(line) for line in preds]))\n",
    "    \n",
    "    return res\n",
    "\n",
    "evaluate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(evaluations, labels, metric='Sensitivity'):\n",
    "    methods = evaluations[0].keys()\n",
    "    evaluations = [[results[metric] for method, results in evaluation.items()] for evaluation in evaluations]\n",
    "    fig, ax = plt.subplots()\n",
    "    ind = np.arange(len(evaluations[0]))\n",
    "    width = 0.2\n",
    "    \n",
    "    p = [ax.bar(ind + i*width, height=evaluation, width=width, bottom=0) for i, evaluation in enumerate(evaluations)]\n",
    "    \n",
    "    ax.set_title(f'{metric} Scores')\n",
    "    ax.set_xticks(ind + width / len(evaluations) * (len(evaluations) - 1))\n",
    "    ax.set_xticklabels(methods)\n",
    "\n",
    "    ax.legend(labels, loc='lower right', fancybox=True, shadow=True)\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standardization vs Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = data.copy()\n",
    "normal = data.copy()\n",
    "for c in data.columns[1:11]:\n",
    "    standard[c] = (data[c] - data[c].mean()) / data[c].std()\n",
    "    normal[c]   = (data[c] - data[c].mean()) / (data[c].max() - data[c].min())\n",
    "standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_progress(evaluations=[evaluate(data), evaluate(standard), evaluate(normal)], \n",
    "              labels=['Original data', 'Standardized', 'Normalized'], \n",
    "              metric='Sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, the step improved the performance of the model a lot for Naive Bayes and KNN.\n",
    "* The performance changes from data standardization to data normalization is not significant\n",
    "* One performs better for Decision Tree, another for Random forest.\n",
    "* So, we will keep data standardization as part of our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Oversampling vs Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "def oversampled_data_split(df, test_size=0.3):\n",
    "    X = df.loc[:, ~df.columns.isin(['class'])]\n",
    "    y = df.loc[:, df.columns.isin(['class'])]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values.flatten(), test_size=0.3, random_state=42)\n",
    "    X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def oversampled_ADASYN_data_split(df, test_size=0.3):\n",
    "    X = df.loc[:, ~df.columns.isin(['class'])]\n",
    "    y = df.loc[:, df.columns.isin(['class'])]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values.flatten(), test_size=0.3, random_state=42)\n",
    "    X_train, y_train = ADASYN().fit_resample(X_train, y_train)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def undersampled_data_split(df, test_size=0.3):\n",
    "    X = df.loc[:, ~df.columns.isin(['class'])]\n",
    "    y = df.loc[:, df.columns.isin(['class'])]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values.flatten(), test_size=0.3, random_state=42)\n",
    "    X_train, y_train = RepeatedEditedNearestNeighbours().fit_resample(X_train, y_train)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_progress(evaluations=[evaluate(data), \n",
    "                           evaluate(data, split_dataset=oversampled_data_split), \n",
    "                           evaluate(data, split_dataset=oversampled_ADASYN_data_split), \n",
    "                           evaluate(data, split_dataset=undersampled_data_split),\n",
    "                          ], \n",
    "              labels=['Original split', 'SMOTE oversampling', 'ADASYN oversampling', 'Undersampling'], \n",
    "              metric='Sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see SMOTE oversampling performed the best among all the other methods\n",
    "* So, we will include the SMOTE oversampling in our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inter_class_important_column_names(df, correlation_threshold=0, plot_correlations=True):\n",
    "    superclasses = list(dict.fromkeys([c.split('-')[0] for c in df.columns]))\n",
    "    groups = {c: [] for c in superclasses}\n",
    "    for c in df.columns:\n",
    "        groups[c.split('-')[0]].append(c)\n",
    "    \n",
    "    important_columns = []\n",
    "    for i, (group_name, columns) in enumerate(groups.items()):\n",
    "        corr_matrix = df[columns].corr().abs()                                               # Calculate the correlation within group\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))  # Select upper triangle of correlation matrix\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "        to_keep = list(set(columns) - set(to_drop))\n",
    "        important_columns += to_keep\n",
    "        \n",
    "        # Plot the correlation\n",
    "        if not plot_correlations or i == 0 or i == len(groups.values()) - 1 or len(columns) > 10:\n",
    "            continue\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 30))\n",
    "        axes[0].set_title(f'In-group correlation for {group_name}')\n",
    "        sns.heatmap(corr_matrix, ax=axes[0], xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True, cmap='Blues', square=True)\n",
    "        \n",
    "        corr_matrix = df[to_keep].corr().abs()\n",
    "        axes[1].set_title('Correlation between the kept variables')\n",
    "        sns.heatmap(corr_matrix, ax=axes[1], xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True, cmap='Blues', square=True)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    return important_columns\n",
    "\n",
    "\n",
    "def get_intra_class_important_column_names(df, correlation_threshold=0, plot_correlations=False):\n",
    "    columns = df.columns\n",
    "    corr_matrix = df[columns].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))  # Select upper triangle of correlation matrix\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "    to_keep = list(set(columns) - set(to_drop) | set(['class']))\n",
    "    return to_keep\n",
    "    \n",
    "\n",
    "importants = get_inter_class_important_column_names(data, correlation_threshold=0.4, plot_correlations=False)\n",
    "importants = data[importants]\n",
    "print(importants.shape)\n",
    "importants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiline_measure_line_chart(ax: plt.Axes, xvalues: list, yvalues: dict, title: str, xlabel: str, ylabel: str, percentage=False, plot_legend=True):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if percentage:\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "    legend = []\n",
    "    colors = {}\n",
    "    for name, y in yvalues.items():\n",
    "        [method, measure] = name.split('-')\n",
    "        style = ':' if measure == 'Sensitivity' else '-'\n",
    "        c = colors.get(method, None)\n",
    "        p = ax.plot(xvalues, y, linestyle=style, c=c)\n",
    "        colors[method] = p[-1].get_color()\n",
    "        legend.append(name)\n",
    "    \n",
    "    if plot_legend:\n",
    "        ax.legend(legend, loc='lower left', fancybox=True, shadow=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold_performance(df, correlation_thresholds, get_importants=get_inter_class_important_column_names,\n",
    "                                   title='Performance of different classifiers', xlabel='Correlation Threshold'):\n",
    "    performance = {}\n",
    "\n",
    "    correlation_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.92, 0.95, 0.97, 1.,]\n",
    "    for threshold in correlation_thresholds:\n",
    "        importants = get_importants(data, correlation_threshold=threshold, plot_correlations=False)\n",
    "        importants = df[importants]\n",
    "\n",
    "        res = evaluate(importants, split_dataset=oversampled_data_split)\n",
    "        for method, measures in res.items():\n",
    "            for measure, value in measures.items():\n",
    "                if measure not in {'Accuracy', 'Sensitivity'}:\n",
    "                    continue\n",
    "                name = f'{method}-{measure}'\n",
    "                if name not in performance:\n",
    "                    performance[name] = []\n",
    "                performance[name].append(value)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xticks(correlation_thresholds)\n",
    "    multiline_measure_line_chart(plt.gca(), correlation_thresholds, performance, title=title, xlabel=xlabel, ylabel='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold_performance(data, \n",
    "                               correlation_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.92, 0.95, 0.97, 1.,], \n",
    "                               get_importants=get_inter_class_important_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On average, the best performance is obtained at threshold close to 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold_performance(data, \n",
    "                               correlation_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.92, 0.95, 0.97, 1.,], \n",
    "                               get_importants=get_intra_class_important_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On average, the best performance is obtained at threshold close to 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we know which are the best thresholds for those two methods lets compare them to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_progress(\n",
    "    evaluations=[\n",
    "        evaluate(data[get_inter_class_important_column_names(data, correlation_threshold=0.95, plot_correlations=False)], split_dataset=oversampled_data_split), \n",
    "        evaluate(data[get_intra_class_important_column_names(data, correlation_threshold=0.8, plot_correlations=False)], split_dataset=oversampled_data_split), \n",
    "    ], \n",
    "    labels=['Inter class', 'Intra class'], \n",
    "    metric='Sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see these two methods have very similar performance\n",
    "* As in most of the cases Inter-class method outperforms slightly the intra-class ones, we will include selecting features based on the inter-class performance in our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis\n",
    "As we don't know much about the variables and it's would be almost impossible to analyze the behaviour of all the 755 variables let's select some important ones and do exploratory analysis on them.\n",
    "\n",
    "We will select the most important features based on the decision tree classifier and do some analysis on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def get_decision_importants(df, nb_importants=6):\n",
    "    X_train, X_test, y_train, y_test = oversampled_data_split(df)\n",
    "    tree = DecisionTreeClassifier(class_weight='balanced')\n",
    "    tree.fit(X_train, y_train)\n",
    "    col2importance = dict(zip(df.columns, tree.feature_importances_))\n",
    "    best = sorted(col2importance.items(), key=operator.itemgetter(1), reverse=True)[:nb_importants]\n",
    "\n",
    "    return [name for name, score in best]\n",
    "\n",
    "importants = get_decision_importants(data, nb_importants=8)\n",
    "importants = data[importants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as _stats \n",
    "def compute_known_distributions(x_values, n_bins) -> dict:\n",
    "    distributions = dict()\n",
    "    # Gaussian\n",
    "    mean, sigma = _stats.norm.fit(x_values)\n",
    "    distributions['Normal(%.1f,%.2f)'%(mean,sigma)] = _stats.norm.pdf(x_values, mean, sigma)\n",
    "    # LogNorm\n",
    "#     sigma, loc, scale = _stats.lognorm.fit(x_values)\n",
    "#     distributions['LogNor(%.1f,%.2f)'%(np.log(scale),sigma)] = _stats.lognorm.pdf(x_values, sigma, loc, scale)\n",
    "    # Exponential\n",
    "#     loc, scale = _stats.expon.fit(x_values)\n",
    "#     distributions['Exp(%.2f)'%(1/scale)] = _stats.expon.pdf(x_values, loc, scale)\n",
    "    # SkewNorm\n",
    "    a, loc, scale = _stats.skewnorm.fit(x_values)\n",
    "    distributions['SkewNorm(%.2f)'%a] = _stats.skewnorm.pdf(x_values, a, loc, scale) \n",
    "    return distributions\n",
    "\n",
    "def histogram_with_distributions(ax: plt.Axes, series: pd.Series, labels: pd.Series, var: str):\n",
    "    classes = labels.unique()[::-1]\n",
    "    for c in classes:\n",
    "        values = pd.Series([s for s, l in zip(series, labels) if l == c])\n",
    "        values = values.sort_values().values\n",
    "        n, bins, patches = ax.hist(values, 20, density=True, edgecolor='grey', label=f'Class {c}')\n",
    "    values = series.sort_values().values\n",
    "    distributions = compute_known_distributions(values, bins)\n",
    "    func.multiple_line_chart(ax, values, distributions, 'Best fit plot', var, 'probability', plot_legend=False)\n",
    "    ax.legend()\n",
    "\n",
    "columns = importants.select_dtypes(include='number').columns\n",
    "rows, cols = func.choose_grid(len(columns) - 1)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(cols*4, rows*4), squeeze=False)\n",
    "i, j = 0, 0\n",
    "for n in range(len(columns)):\n",
    "    histogram_with_distributions(axs[i, j], data[columns[n]].dropna(), data['class'], columns[n])\n",
    "    i, j = (i + 1, 0) if (n+1) % cols == 0 else (i, j + 1)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = importants.select_dtypes(include='number').columns\n",
    "rows, cols = func.choose_grid(len(columns) - 1)\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(cols*4, rows*4), squeeze=False)\n",
    "i, j = 0, 0\n",
    "for n in range(len(columns)):\n",
    "    axs[i, j].set_title('Boxplot for %s'%columns[n])\n",
    "    axs[i, j].boxplot(data[columns[n]].dropna().values)\n",
    "    i, j = (i + 1, 0) if (n+1) % cols == 0 else (i, j + 1)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Variate Analysis\n",
    "##### Again, as the dataset is too large in terms of the number of columns, we will concentrate on the variables that are important based the previous findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = importants.select_dtypes(include='number').columns\n",
    "\n",
    "rows, cols = len(columns)-1, len(columns)-1\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(cols*4, rows*4), squeeze=False)\n",
    "for i in range(len(columns)):\n",
    "    var1 = columns[i]\n",
    "    for j in range(i+1, len(columns)):\n",
    "        var2 = columns[j]\n",
    "        axs[i, j-1].set_title(\"%s x %s\"%(var1,var2))\n",
    "        axs[i, j-1].set_xlabel(var1)\n",
    "        axs[i, j-1].set_ylabel(var2)\n",
    "        axs[i, j-1].scatter(data[var1], data[var2])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 12])\n",
    "corr_mtx = importants.corr()\n",
    "sns.heatmap(corr_mtx, xticklabels=corr_mtx.columns, yticklabels=corr_mtx.columns, annot=True, cmap='Blues', square=True)\n",
    "plt.title('Correlation analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = oversampled_data_split(data)\n",
    "class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "print('Class weights:', class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "# print('ROC AUC:', metrics.roc_auc_score(y_test, y_pred))\n",
    "print('Sensitivity:', metrics.recall_score(y_test, y_pred))\n",
    "cnf_mtx = metrics.confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cnf_mtx, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nvalues = [1, 3, 5, 7, 9, 11, 13]\n",
    "dist = ['manhattan', 'euclidean', 'chebyshev', 'minkowski']\n",
    "\n",
    "performance = {}\n",
    "for d in dist:\n",
    "    performance[f'{d}-ROC AUC'], performance[f'{d}-Sensitivity'] = [], []\n",
    "    for n in nvalues:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n, metric=d)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        performance[f'{d}-Accuracy'].append(metrics.accuracy_score(y_test, y_pred))\n",
    "#         performance[f'{d}-ROC AUC'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "        performance[f'{d}-Sensitivity'].append(metrics.recall_score(y_test, y_pred))\n",
    "    acc = performance[f'{d}-Accuracy']\n",
    "#     auc = performance[f'{d}-ROC AUC']\n",
    "    sens = performance[f'{d}-Sensitivity']\n",
    "    print(f'[d={d}] Best acc: {max(acc)}, with nvalues: {nvalues[np.argmax(acc)]}')\n",
    "#     print(f'[d={d}] Best auc: {max(auc)}, with nvalues: {nvalues[np.argmax(auc)]}')\n",
    "    print(f'[d={d}] Best sensitivity: {max(sens)}, with nvalues: {nvalues[np.argmax(sens)]}')\n",
    "    print('---------------------------')\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks(nvalues)\n",
    "multiline_measure_line_chart(plt.gca(), nvalues, performance, title='KNN variants', xlabel='k', ylabel='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = np.array(range(1, 200, 10)) / 1000.\n",
    "print('Min samples leaf:', min_samples_leaf)\n",
    "max_depths = [3, 5, 7, 10, 15]\n",
    "criteria = ['entropy', 'gini']\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), squeeze=False)\n",
    "for k, f in enumerate(criteria):\n",
    "    performance = {}\n",
    "    for d in max_depths:\n",
    "        performance[f'd={d}-ROC AUC'], performance[f'd={d}-Sensitivity'] = [], []\n",
    "        for n in min_samples_leaf:\n",
    "            tree = DecisionTreeClassifier(min_samples_leaf=n, max_depth=d, criterion=f, class_weight='balanced')\n",
    "            tree.fit(X_train, y_train)\n",
    "            y_pred = tree.predict(X_test)\n",
    "            performance[f'd={d}-Accuracy'].append(metrics.accuracy_score(y_test, y_pred))\n",
    "#             performance[f'd={d}-ROC AUC'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            performance[f'd={d}-Sensitivity'].append(metrics.recall_score(y_test, y_pred))\n",
    "        acc = performance[f'd={d}-Accuracy']\n",
    "#         auc = performance[f'd={d}-ROC AUC']\n",
    "        sens = performance[f'd={d}-Sensitivity']\n",
    "            \n",
    "        print(f'[criterion={f}, d={d}] Best acc: {max(acc)}, with min_samples_leaf: {min_samples_leaf[np.argmax(acc)]}')\n",
    "#         print(f'[criterion={f}, d={d}] Best auc: {max(auc)}, with min_samples_leaf: {min_samples_leaf[np.argmax(auc)]}')\n",
    "        print(f'[criterion={f}, d={d}] Best sensitivity: {max(sens)}, with min_samples_leaf: {min_samples_leaf[np.argmax(sens)]}')\n",
    "        print('---------------------------')\n",
    "    multiline_measure_line_chart(axs[0, k], min_samples_leaf, performance, title=f'{f} variants', xlabel='min_samples_leaf', ylabel='', percentage=True)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200]\n",
    "max_depths = [3, 5, 7, 10, 15]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, len(max_features), figsize=(16, 6), squeeze=False)\n",
    "for k, f in enumerate(max_features):\n",
    "    performance = {}\n",
    "\n",
    "    for d in max_depths:\n",
    "        performance[f'd={d}-ROC AUC'], performance[f'd={d}-Sensitivity'] = [], []\n",
    "        for n in n_estimators:\n",
    "            rf = RandomForestClassifier(n_estimators=n, max_depth=d, max_features=f, class_weight='balanced')\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            performance[f'd={d}-Accuracy'].append(metrics.accuracy_score(y_test, y_pred))\n",
    "#             performance[f'd={d}-ROC AUC'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "            performance[f'd={d}-Sensitivity'].append(metrics.recall_score(y_test, y_pred))\n",
    "        acc = performance[f'd={d}-Accuracy']\n",
    "#         auc = performance[f'd={d}-ROC AUC']\n",
    "        sens = performance[f'd={d}-Sensitivity']\n",
    "            \n",
    "        print(f'[max_features={k}, d={d}] Best acc: {max(acc)}, with n_estimators: {n_estimators[np.argmax(acc)]}')\n",
    "#         print(f'[max_features={f}, d={d}] Best auc: {max(auc)}, with n_estimators: {n_estimators[np.argmax(auc)]}')\n",
    "        print(f'[max_features={f}, d={d}] Best sensitivity: {max(sens)}, with n_estimators: {n_estimators[np.argmax(sens)]}')\n",
    "        print('---------------------------')\n",
    "    multiline_measure_line_chart(axs[0, k], n_estimators, performance, title=f'{f} variants', xlabel='nnumber of estimators', ylabel='', percentage=True)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = list(range(5, 51, 5))\n",
    "max_depths = [3, 5, 7, 10, 15]\n",
    "\n",
    "performance = {}\n",
    "for d in max_depths:\n",
    "    performance[f'd={d}-ROC AUC'], performance[f'd={d}-Sensitivity'] = [], []\n",
    "    for n in nb_steps:\n",
    "        param = {\n",
    "            'eta': 0.3, \n",
    "            'max_depth': d,  \n",
    "            'objective': 'multi:softprob',  \n",
    "            'num_class': len(target_count)\n",
    "        } \n",
    "        model = xgb.train(param, D_train, n)\n",
    "        preds = model.predict(D_test)\n",
    "        y_pred = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "        performance[f'd={d}-Accuracy'].append(metrics.accuracy_score(y_test, y_pred))\n",
    "#         performance[f'd={d}-ROC AUC'].append(metrics.roc_auc_score(y_test, y_pred))\n",
    "        performance[f'd={d}-Sensitivity'].append(metrics.recall_score(y_test, y_pred))\n",
    "    acc = performance[f'd={d}-Accuracy']\n",
    "#     auc = performance[f'd={d}-ROC AUC']\n",
    "    sens = performance[f'd={d}-Sensitivity']\n",
    "    print(f'[d={d}] Best acc: {max(acc)}, with nvalues: {nvalues[np.argmax(acc)]}')\n",
    "#     print(f'[d={d}] Best auc: {max(auc)}, with nb_steps: {nb_steps[np.argmax(auc)]}')\n",
    "    print(f'[d={d}] Best sensitivity: {max(sens)}, with nb_steps: {nb_steps[np.argmax(sens)]}')\n",
    "    print('---------------------------')\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks(nb_steps)\n",
    "multiline_measure_line_chart(plt.gca(), nb_steps, performance, title='xgboost variants', xlabel='nb_steps', ylabel='', percentage=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 5,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': len(target_count),\n",
    "}\n",
    "\n",
    "boost = xgb.train(param, D_train, 30)\n",
    "res = evaluate(data, split_dataset=oversampled_data_split, \n",
    "         knn=KNeighborsClassifier(n_neighbors=1, metric='manhattan'), \n",
    "         tree=DecisionTreeClassifier(min_samples_leaf=0.001, max_depth=7, criterion='entropy', class_weight='balanced'), \n",
    "         rf=RandomForestClassifier(n_estimators=50, max_depth=15, max_features='log2', class_weight='balanced'), \n",
    "         boost=boost)\n",
    "\n",
    "pprint(res)\n",
    "show_progress(\n",
    "    evaluations=[res], \n",
    "    labels=['Best models'], \n",
    "    metric='ROC AUC'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_progress(\n",
    "    evaluations=[res], \n",
    "    labels=['Best models'], \n",
    "    metric='Sensitivity'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, the KNN model with n_neighbors=1, metric='manhattan' has the best performance in terms of the AUC\n",
    "* In terms of the Accuracy and sensitivity it has relatively the same performance with the Random Forest and xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.cut\n",
    "selected_columns = get_inter_class_important_column_names(data, correlation_threshold=0.1, plot_correlations=False)\n",
    "selected_columns = list(set(selected_columns + ['class']))\n",
    "\n",
    "newdf = data[selected_columns].copy()\n",
    "for col in newdf:\n",
    "    if col not in ['class']: \n",
    "        newdf[col] = pd.cut(newdf[col],3,labels=['0','1','2'])\n",
    "newdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummylist = []\n",
    "for att in newdf:\n",
    "    if att in ['class'] or att.startswith('y'): newdf[att] = newdf[att].astype('category')\n",
    "    dummylist.append(pd.get_dummies(newdf[[att]]))\n",
    "dummified_df = pd.concat(dummylist, axis=1)\n",
    "dummified_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(dummified_df, min_support=0.6, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "rules[(rules['antecedent_len']>=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {'Sum of squares': []}\n",
    "ks = list(range(1, 33))\n",
    "for k in ks:\n",
    "    X = data.loc[:, ~data.columns.isin(['class'])]\n",
    "    kmeans_model = cluster.KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    errors['Sum of squares'].append(kmeans_model.inertia_)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xticks(ks)\n",
    "func.multiple_line_chart(plt.gca(), ks, errors, 'K means', 'k', 'Sum of squares error', plot_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* According to the elbow method the best number of clusters is 3 or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {'auto', 'ball_tree', 'kd_tree', 'brute'}\n",
    "leaf_sizes = list(range(2, 50, 2))\n",
    "\n",
    "performance = {}\n",
    "for algo in algorithms:\n",
    "    performance[algo] = []\n",
    "    for l in leaf_sizes:\n",
    "        X = data.loc[:, ~data.columns.isin(['class'])]\n",
    "        db = DBSCAN(eps=0.5, min_samples=2, algorithm=algo, leaf_size=l)\n",
    "        labels = db.fit_predict(X)\n",
    "        performance[algo].append(metrics.silhouette_score(X, labels))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xticks(min_samples)\n",
    "func.multiple_line_chart(plt.gca(), min_samples, performance, 'DBSCAN', 'leaf size', 'Silhouette score', plot_legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
